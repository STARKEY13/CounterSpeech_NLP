{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11488033,"sourceType":"datasetVersion","datasetId":7200788},{"sourceId":11488038,"sourceType":"datasetVersion","datasetId":7200792},{"sourceId":11488144,"sourceType":"datasetVersion","datasetId":7200876}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets evaluate bert_score detoxify","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BLOOMZ WITH NO FINE TUNE","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset, DatasetDict\nimport numpy as np\nfrom evaluate import load\n\n# 1. Load and prepare dataset\ndataset = load_dataset(\"Rhma/DIALOCONAN\")\nsmall_dataset = dataset[\"train\"].select(range(3500))\n\n# Split train data into train/validation\ntrain_val = small_dataset.train_test_split(test_size=0.15, seed=42)\ndataset = DatasetDict({\n    \"train\": train_val[\"train\"],\n    \"validation\": train_val[\"test\"]\n})\n\n# 2. Group turns by dialogue_id\ndef group_dialogues(examples):\n    sorted_data = sorted(zip(examples[\"dialogue_id\"], \n                            examples[\"turn_id\"], \n                            examples[\"text\"],\n                            examples[\"type\"],\n                            examples[\"TARGET\"]),\n                       key=lambda x: (x[0], x[1]))\n    dialogues = []\n    current_dialogue = []\n    current_id = None\n    for item in sorted_data:\n        dialogue_id, turn_id, text, turn_type, target = item\n        if dialogue_id != current_id:\n            if current_id is not None and current_dialogue:\n                dialogues.append({\n                    \"dialogue_id\": current_id,\n                    \"turns\": current_dialogue,\n                    \"target\": current_dialogue[0][\"target\"]\n                })\n            current_id = dialogue_id\n            current_dialogue = []\n        current_dialogue.append({\n            \"text\": text,\n            \"type\": turn_type,\n            \"target\": target\n        })\n    if current_id is not None and current_dialogue:\n        dialogues.append({\n            \"dialogue_id\": current_id,\n            \"turns\": current_dialogue,\n            \"target\": current_dialogue[0][\"target\"]\n        })\n    return {\"dialogues\": dialogues}\n\nprocessed_dataset = dataset.map(\n    group_dialogues,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    batch_size=1000\n)\n\n# 3. Create conversation history for each CN turn\ndef create_conversation_history(examples):\n    new_examples = {\"input\": [], \"target\": []}\n    for dialogue in examples[\"dialogues\"]:\n        history = []\n        for turn in dialogue[\"turns\"]:\n            if turn[\"type\"] == \"CN\":\n                new_examples[\"input\"].append(\" [SEP] \".join(history))\n                new_examples[\"target\"].append(turn[\"text\"])\n            history.append(turn[\"text\"])\n    return new_examples\n\nfinal_dataset = processed_dataset.map(\n    create_conversation_history,\n    batched=True,\n    remove_columns=[\"dialogues\"]\n)\n\n# 4. Load pretrained BLOOMZ model and tokenizer\nmodel_name = \"bigscience/bloomz-3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n\n# 5. Generation function (inference only)\ndef generate_counterspeech(dialogue_history):\n    device = model.device\n    input_text = \" [SEP] \".join(dialogue_history) + \" [ANS] \"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            inputs.input_ids,\n            max_new_tokens=128,\n            num_beams=5,\n            repetition_penalty=2.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 6. Example usage\nsample_dialogue = [\n    \"You people are ruining our country!\",\n    \"Immigrants are stealing our jobs!\",\n    \"We should send them all back!\"\n]\nprint(\"\\nGenerated counterspeech:\")\nprint(generate_counterspeech(sample_dialogue))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BLOOMZ WITH INSTRUCTION FINE TUNING","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom datasets import load_dataset, DatasetDict\nfrom peft import LoraConfig, get_peft_model, TaskType\nimport numpy as np\nfrom evaluate import load\n\n# 1. Load and prepare dataset\ndataset = load_dataset(\"Rhma/DIALOCONAN\")\nsmall_dataset = dataset[\"train\"].select(range(3500))\ntrain_val = small_dataset.train_test_split(test_size=0.15, seed=42)\ndataset = DatasetDict({\n    \"train\": train_val[\"train\"],\n    \"validation\": train_val[\"test\"]\n})\n\n# 2. Group turns by dialogue_id\ndef group_dialogues(examples):\n    sorted_data = sorted(zip(examples[\"dialogue_id\"], \n                            examples[\"turn_id\"], \n                            examples[\"text\"],\n                            examples[\"type\"],\n                            examples[\"TARGET\"]),\n                       key=lambda x: (x[0], x[1]))\n    dialogues = []\n    current_dialogue = []\n    current_id = None\n    for item in sorted_data:\n        dialogue_id, turn_id, text, turn_type, target = item\n        if dialogue_id != current_id:\n            if current_id is not None and current_dialogue:\n                dialogues.append({\n                    \"dialogue_id\": current_id,\n                    \"turns\": current_dialogue,\n                    \"target\": current_dialogue[0][\"target\"]\n                })\n            current_id = dialogue_id\n            current_dialogue = []\n        current_dialogue.append({\n            \"text\": text,\n            \"type\": turn_type,\n            \"target\": target\n        })\n    if current_id is not None and current_dialogue:\n        dialogues.append({\n            \"dialogue_id\": current_id,\n            \"turns\": current_dialogue,\n            \"target\": current_dialogue[0][\"target\"]\n        })\n    return {\"dialogues\": dialogues}\n\nprocessed_dataset = dataset.map(\n    group_dialogues,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    batch_size=1000\n)\n\n# 3. Create conversation history for each CN turn\ndef create_conversation_history(examples):\n    new_examples = {\"input\": [], \"target\": []}\n    for dialogue in examples[\"dialogues\"]:\n        history = []\n        for turn in dialogue[\"turns\"]:\n            if turn[\"type\"] == \"CN\":\n                new_examples[\"input\"].append(\" [SEP] \".join(history))\n                new_examples[\"target\"].append(turn[\"text\"])\n            history.append(turn[\"text\"])\n    return new_examples\n\nfinal_dataset = processed_dataset.map(\n    create_conversation_history,\n    batched=True,\n    remove_columns=[\"dialogues\"]\n)\n\n# 4. Instruction tuning formatting\ndef format_instruction(examples):\n    inputs = []\n    targets = []\n    for inp, tgt in zip(examples[\"input\"], examples[\"target\"]):\n        instruction = (\n            \"[INST] <<SYS>>\\n\"\n            \"You are a helpful assistant that generates fact-based counterspeech.\"\n            \"<<SYS>>\\n\"\n            f\"{inp} [/INST]\"\n        )\n        inputs.append(instruction)\n        targets.append(tgt)\n    return {\"input\": inputs, \"target\": targets}\n\ninstruction_dataset = final_dataset.map(\n    format_instruction,\n    batched=True,\n    remove_columns=[\"input\", \"target\"]\n)\n\n# 5. Tokenization\nmodel_name = \"bigscience/bloomz-3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\n\ndef preprocess_function(examples):\n    inputs = [inp + \" \" + tgt for inp, tgt in zip(examples[\"input\"], examples[\"target\"])]\n    model_inputs = tokenizer(\n        inputs,\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n\ntokenized_datasets = instruction_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=[\"input\", \"target\"]\n)\n\n# 6. LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"query_key_value\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.CAUSAL_LM,\n)\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# 7. TrainingArguments\ntraining_args = TrainingArguments(\n    output_dir=\"./bloomz-lora-instruction\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=2,\n    fp16=True,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    report_to=\"none\",\n    eval_accumulation_steps=1,\n    learning_rate=2e-4\n)\n\n\n# 9. Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer\n    \n)\n\nprint(\"Starting instruction tuning training...\")\ntrainer.train()\n\n# 10. Generation function (for inference)\ndef generate_counterspeech(dialogue_history):\n    device = model.device\n    input_text = \"[INST] <<SYS>>\\nYou are a helpful assistant that generates fact-based counterspeech.<<SYS>>\\n\" + \" [SEP] \".join(dialogue_history) + \" [/INST] \"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            inputs.input_ids,\n            max_new_tokens=128,\n            num_beams=5,\n            repetition_penalty=2.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Example usage\nsample_dialogue = [\n    \"You people are ruining our country!\",\n    \"Immigrants are stealing our jobs!\",\n    \"We should send them all back!\"\n]\nprint(\"\\nGenerated counterspeech after fine-tuning:\")\nprint(generate_counterspeech(sample_dialogue))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies if not already installed\n!pip install -q evaluate detoxify tqdm\n!pip install rouge_score bert_score\nfrom evaluate import load\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\n# Load metrics\nrouge = load(\"rouge\")\nbertscore = load(\"bertscore\")\n# Use first 100 samples\ninputs = [ex[\"input\"] for ex in final_dataset[\"validation\"]][:100]\ntargets = [ex[\"target\"] for ex in final_dataset[\"validation\"]][:100]\n\n# Generate predictions\nprint(\"Generating counter speech...\")\ngenerated = []\nfor text in tqdm(inputs, desc=\"Generating\"):\n    response = generate_counterspeech(text)  # <-- make sure this function is defined\n    generated.append(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BERTScore\nprint(\"Calculating BERTScore...\")\nbertscore_result = bertscore.compute(\n    predictions=generated,\n    references=targets,\n    model_type=\"distilbert-base-uncased\"\n)\nprint(f\"BERTScore F1: {np.mean(bertscore_result['f1']):.4f}\")\n\n# ROUGE\n# ROUGE\nprint(\"Calculating ROUGE...\")\nrouge_result = rouge.compute(predictions=generated, references=targets)\nprint(f\"ROUGE-1 F1: {rouge_result['rouge1']:.4f}\")\nprint(f\"ROUGE-2 F1: {rouge_result['rouge2']:.4f}\")\nprint(f\"ROUGE-L F1: {rouge_result['rougeL']:.4f}\")\n# Perplexity\nprint(\"Calculating Perplexity...\")\ndef calculate_perplexity(texts):\n    total_log_prob = 0.0\n    total_words = 0\n    for text in texts:\n        words = text.split()\n        total_words += len(words)\n        # You can use a pre-trained language model (e.g., GPT-2) for calculating perplexity\n        # Here, we will use a placeholder for the log-prob calculation, which should ideally come from a language model\n        # For simplicity, assume a fixed value here\n        total_log_prob += len(words) * math.log(1.0)  # Placeholder for log-prob calculation\n    return math.exp(-total_log_prob / total_words) if total_words > 0 else float('inf')\n\nperplexity_result = calculate_perplexity(generated)\nprint(f\"Perplexity: {perplexity_result:.4f}\")\n\n# Toxicity\nprint(\"Calculating Toxicity...\")\ntoxicity_scores = [Detoxify('original').predict(pred)['toxicity'] for pred in tqdm(generated, desc=\"Toxicity\")]\navg_toxicity = np.mean(toxicity_scores)\nprint(f\"Avg. Toxicity Score: {avg_toxicity:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# BLOOMZ WITH PREFIX OR PROMPT TUNING","metadata":{}},{"cell_type":"code","source":"!pip install peft","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\nfrom datasets import load_dataset, DatasetDict\nimport numpy as np\nfrom evaluate import load\nfrom peft import PrefixTuningConfig, get_peft_model, TaskType\n\n# 1. Load and prepare dataset\ndataset = load_dataset(\"Rhma/DIALOCONAN\")\nsmall_dataset = dataset[\"train\"].select(range(3500))\ntrain_val = small_dataset.train_test_split(test_size=0.15, seed=42)\ndataset = DatasetDict({\n    \"train\": train_val[\"train\"],\n    \"validation\": train_val[\"test\"]\n})\n\n# 2. Group turns by dialogue_id\ndef group_dialogues(examples):\n    sorted_data = sorted(zip(examples[\"dialogue_id\"], \n                            examples[\"turn_id\"], \n                            examples[\"text\"],\n                            examples[\"type\"],\n                            examples[\"TARGET\"]),\n                       key=lambda x: (x[0], x[1]))\n    dialogues = []\n    current_dialogue = []\n    current_id = None\n    for item in sorted_data:\n        dialogue_id, turn_id, text, turn_type, target = item\n        if dialogue_id != current_id:\n            if current_id is not None and current_dialogue:\n                dialogues.append({\n                    \"dialogue_id\": current_id,\n                    \"turns\": current_dialogue,\n                    \"target\": current_dialogue[0][\"target\"]\n                })\n            current_id = dialogue_id\n            current_dialogue = []\n        current_dialogue.append({\n            \"text\": text,\n            \"type\": turn_type,\n            \"target\": target\n        })\n    if current_id is not None and current_dialogue:\n        dialogues.append({\n            \"dialogue_id\": current_id,\n            \"turns\": current_dialogue,\n            \"target\": current_dialogue[0][\"target\"]\n        })\n    return {\"dialogues\": dialogues}\n\nprocessed_dataset = dataset.map(\n    group_dialogues,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    batch_size=1000\n)\n\n# 3. Create conversation history for each CN turn\ndef create_conversation_history(examples):\n    new_examples = {\"input\": [], \"target\": []}\n    for dialogue in examples[\"dialogues\"]:\n        history = []\n        for turn in dialogue[\"turns\"]:\n            if turn[\"type\"] == \"CN\":\n                new_examples[\"input\"].append(\" [SEP] \".join(history))\n                new_examples[\"target\"].append(turn[\"text\"])\n            history.append(turn[\"text\"])\n    return new_examples\n\nfinal_dataset = processed_dataset.map(\n    create_conversation_history,\n    batched=True,\n    remove_columns=[\"dialogues\"]\n)\n\n# 4. Load pretrained BLOOMZ model and tokenizer\nmodel_name = \"bigscience/bloomz-3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n\n# 5. PEFT: PrefixTuning\npeft_config = PrefixTuningConfig(\n    task_type=TaskType.CAUSAL_LM,\n    num_virtual_tokens=30,  # can be tuned\n    encoder_hidden_size=model.config.hidden_size\n)\n# --- OR PromptTuning ---\n# peft_config = PromptTuningConfig(\n#     task_type=TaskType.CAUSAL_LM,\n#     num_virtual_tokens=30\n# )\nmodel = get_peft_model(model, peft_config)\nmodel.print_trainable_parameters()\n\n# 6. Tokenization for Trainer\ndef preprocess_function(examples):\n    inputs = [inp + \" [ANS] \" + tgt for inp, tgt in zip(examples[\"input\"], examples[\"target\"])]\n    model_inputs = tokenizer(\n        inputs,\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    model_inputs[\"labels\"] = model_inputs[\"input_ids\"].copy()\n    return model_inputs\n\ntokenized_datasets = final_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=[\"input\", \"target\"]\n)\n\n# 7. TrainingArguments and Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"./bloomz-prefix-tuning\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=2,\n    fp16=True,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    report_to=\"none\",\n    eval_accumulation_steps=1,\n    learning_rate=2e-4\n)\n\n\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer\n   \n)\n\nprint(\"Starting PrefixTuning training...\")\ntrainer.train()\n\n# 8. Generation function (for inference)\ndef generate_counterspeech(dialogue_history):\n    device = model.device\n    input_text = \" [SEP] \".join(dialogue_history) + \" [ANS] \"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            inputs.input_ids,\n            max_new_tokens=128,\n            num_beams=5,\n            repetition_penalty=2.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 9. Example usage\nsample_dialogue = [\n    \"You people are ruining our country!\",\n    \"Immigrants are stealing our jobs!\",\n    \"We should send them all back!\"\n]\nprint(\"\\nGenerated counterspeech after PrefixTuning:\")\nprint(generate_counterspeech(sample_dialogue))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies if not already installed\n!pip install -q evaluate detoxify tqdm\n!pip install rouge_score bert_score\nfrom evaluate import load\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\n# Load metrics\nrouge = load(\"rouge\")\nbertscore = load(\"bertscore\")\n# Use first 100 samples\ninputs = [ex[\"input\"] for ex in final_dataset[\"validation\"]][:100]\ntargets = [ex[\"target\"] for ex in final_dataset[\"validation\"]][:100]\n\n# Generate predictions\nprint(\"Generating counter speech...\")\ngenerated = []\nfor text in tqdm(inputs, desc=\"Generating\"):\n    response = generate_counterspeech(text)  # <-- make sure this function is defined\n    generated.append(response)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# BERTScore\nprint(\"Calculating BERTScore...\")\nbertscore_result = bertscore.compute(\n    predictions=generated,\n    references=targets,\n    model_type=\"distilbert-base-uncased\"\n)\nprint(f\"BERTScore F1: {np.mean(bertscore_result['f1']):.4f}\")\n\n# ROUGE\n# ROUGE\nprint(\"Calculating ROUGE...\")\nrouge_result = rouge.compute(predictions=generated, references=targets)\nprint(f\"ROUGE-1 F1: {rouge_result['rouge1']:.4f}\")\nprint(f\"ROUGE-2 F1: {rouge_result['rouge2']:.4f}\")\nprint(f\"ROUGE-L F1: {rouge_result['rougeL']:.4f}\")\n# Perplexity\nprint(\"Calculating Perplexity...\")\ndef calculate_perplexity(texts):\n    total_log_prob = 0.0\n    total_words = 0\n    for text in texts:\n        words = text.split()\n        total_words += len(words)\n        # You can use a pre-trained language model (e.g., GPT-2) for calculating perplexity\n        # Here, we will use a placeholder for the log-prob calculation, which should ideally come from a language model\n        # For simplicity, assume a fixed value here\n        total_log_prob += len(words) * math.log(1.0)  # Placeholder for log-prob calculation\n    return math.exp(-total_log_prob / total_words) if total_words > 0 else float('inf')\n\nperplexity_result = calculate_perplexity(generated)\nprint(f\"Perplexity: {perplexity_result:.4f}\")\n\n# Toxicity\nprint(\"Calculating Toxicity...\")\ntoxicity_scores = [Detoxify('original').predict(pred)['toxicity'] for pred in tqdm(generated, desc=\"Toxicity\")]\navg_toxicity = np.mean(toxicity_scores)\nprint(f\"Avg. Toxicity Score: {avg_toxicity:.4f}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}