{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets evaluate bert_score detoxify","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Flan-t5-xl WITH NO FINE TUNE","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration\nfrom datasets import load_dataset, DatasetDict\nimport numpy as np\n\n# 1. Load and prepare dataset\ndataset = load_dataset(\"Rhma/DIALOCONAN\")\nsmall_dataset = dataset[\"train\"].select(range(3500))\n\n# Split train data into train/validation\ntrain_val = small_dataset.train_test_split(test_size=0.15, seed=42)\ndataset = DatasetDict({\n    \"train\": train_val[\"train\"],\n    \"validation\": train_val[\"test\"]\n})\n\n# 2. Group turns by dialogue_id\ndef group_dialogues(examples):\n    sorted_data = sorted(zip(examples[\"dialogue_id\"], \n                            examples[\"turn_id\"], \n                            examples[\"text\"],\n                            examples[\"type\"],\n                            examples[\"TARGET\"]),\n                       key=lambda x: (x[0], x[1]))\n    dialogues = []\n    current_dialogue = []\n    current_id = None\n    for item in sorted_data:\n        dialogue_id, turn_id, text, turn_type, target = item\n        if dialogue_id != current_id:\n            if current_id is not None and current_dialogue:\n                dialogues.append({\n                    \"dialogue_id\": current_id,\n                    \"turns\": current_dialogue,\n                    \"target\": current_dialogue[0][\"target\"]\n                })\n            current_id = dialogue_id\n            current_dialogue = []\n        current_dialogue.append({\n            \"text\": text,\n            \"type\": turn_type,\n            \"target\": target\n        })\n    if current_id is not None and current_dialogue:\n        dialogues.append({\n            \"dialogue_id\": current_id,\n            \"turns\": current_dialogue,\n            \"target\": current_dialogue[0][\"target\"]\n        })\n    return {\"dialogues\": dialogues}\n\nprocessed_dataset = dataset.map(\n    group_dialogues,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    batch_size=1000\n)\n\n# 3. Create conversation history for each CN turn\ndef create_conversation_history(examples):\n    new_examples = {\"input\": [], \"target\": []}\n    for dialogue in examples[\"dialogues\"]:\n        history = []\n        for turn in dialogue[\"turns\"]:\n            if turn[\"type\"] == \"CN\":\n                new_examples[\"input\"].append(\" [SEP] \".join(history))\n                new_examples[\"target\"].append(turn[\"text\"])\n            history.append(turn[\"text\"])\n    return new_examples\n\nfinal_dataset = processed_dataset.map(\n    create_conversation_history,\n    batched=True,\n    remove_columns=[\"dialogues\"]\n)\n\n# 4. Load pretrained FLAN-T5-XL model and tokenizer\nmodel_name = \"google/flan-t5-xl\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n\n# 5. Generation function (inference only)\ndef generate_counterspeech(dialogue_history):\n    device = model.device\n    # Instruction-style prompt for T5 (you can tweak this)\n    input_text = \"Given the following conversation, generate a fact-based counterspeech response:\\n\" + \" [SEP] \".join(dialogue_history)\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            inputs.input_ids,\n            max_new_tokens=128,\n            num_beams=5,\n            repetition_penalty=2.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 6. Example usage\nsample_dialogue = [\n    \"You people are ruining our country!\",\n    \"Immigrants are stealing our jobs!\",\n    \"We should send them all back!\"\n]\nprint(\"\\nGenerated counterspeech (FLAN-T5-XL):\")\nprint(generate_counterspeech(sample_dialogue))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# WITH LORA FINE TUNE","metadata":{}},{"cell_type":"code","source":"# 0. Install PEFT if not already installed (uncomment if needed)\n!pip install peft\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\nfrom datasets import load_dataset, DatasetDict\nimport numpy as np\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# 1. Load and prepare dataset\ndataset = load_dataset(\"Rhma/DIALOCONAN\")\nsmall_dataset = dataset[\"train\"].select(range(3500))\ntrain_val = small_dataset.train_test_split(test_size=0.15, seed=42)\ndataset = DatasetDict({\n    \"train\": train_val[\"train\"],\n    \"validation\": train_val[\"test\"]\n})\n\n# 2. Group turns by dialogue_id\ndef group_dialogues(examples):\n    sorted_data = sorted(zip(examples[\"dialogue_id\"], \n                            examples[\"turn_id\"], \n                            examples[\"text\"],\n                            examples[\"type\"],\n                            examples[\"TARGET\"]),\n                       key=lambda x: (x[0], x[1]))\n    dialogues = []\n    current_dialogue = []\n    current_id = None\n    for item in sorted_data:\n        dialogue_id, turn_id, text, turn_type, target = item\n        if dialogue_id != current_id:\n            if current_id is not None and current_dialogue:\n                dialogues.append({\n                    \"dialogue_id\": current_id,\n                    \"turns\": current_dialogue,\n                    \"target\": current_dialogue[0][\"target\"]\n                })\n            current_id = dialogue_id\n            current_dialogue = []\n        current_dialogue.append({\n            \"text\": text,\n            \"type\": turn_type,\n            \"target\": target\n        })\n    if current_id is not None and current_dialogue:\n        dialogues.append({\n            \"dialogue_id\": current_id,\n            \"turns\": current_dialogue,\n            \"target\": current_dialogue[0][\"target\"]\n        })\n    return {\"dialogues\": dialogues}\n\nprocessed_dataset = dataset.map(\n    group_dialogues,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    batch_size=1000\n)\n\n# 3. Create conversation history for each CN turn\ndef create_conversation_history(examples):\n    new_examples = {\"input\": [], \"target\": []}\n    for dialogue in examples[\"dialogues\"]:\n        history = []\n        for turn in dialogue[\"turns\"]:\n            if turn[\"type\"] == \"CN\":\n                new_examples[\"input\"].append(\" [SEP] \".join(history))\n                new_examples[\"target\"].append(turn[\"text\"])\n            history.append(turn[\"text\"])\n    return new_examples\n\nfinal_dataset = processed_dataset.map(\n    create_conversation_history,\n    batched=True,\n    remove_columns=[\"dialogues\"]\n)\n\n# 4. Load pretrained FLAN-T5-XL model and tokenizer\nmodel_name = \"google/flan-t5-xl\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n\n# 5. Apply LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q\", \"k\", \"v\"],  # T5 attention modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# 6. Tokenization for Trainer\ndef preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples[\"input\"],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    labels = tokenizer(\n        examples[\"target\"],\n        max_length=64,\n        truncation=True,\n        padding=\"max_length\"\n    )[\"input_ids\"]\n    labels = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n        for label_seq in labels\n    ]\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\ntokenized_datasets = final_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=[\"input\", \"target\"]\n)\n\n# 7. TrainingArguments and Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"./flan-t5-xl-lora-counterspeech\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=2,\n    fp16=False,\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    report_to=\"none\",\n    learning_rate=2e-4\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer\n)\n\nprint(\"Starting LoRA fine-tuning...\")\ntrainer.train()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# INSTRUCTION TUNED","metadata":{}},{"cell_type":"code","source":"# !pip install peft\n\nimport torch\nfrom transformers import T5Tokenizer, T5ForConditionalGeneration, TrainingArguments, Trainer\nfrom datasets import load_dataset, DatasetDict\nimport numpy as np\nfrom peft import LoraConfig, get_peft_model, TaskType\n\n# 1. Load and prepare dataset\ndataset = load_dataset(\"Rhma/DIALOCONAN\")\nsmall_dataset = dataset[\"train\"].select(range(3500))\ntrain_val = small_dataset.train_test_split(test_size=0.15, seed=42)\ndataset = DatasetDict({\n    \"train\": train_val[\"train\"],\n    \"validation\": train_val[\"test\"]\n})\n\n# 2. Group turns by dialogue_id\ndef group_dialogues(examples):\n    sorted_data = sorted(zip(examples[\"dialogue_id\"], \n                            examples[\"turn_id\"], \n                            examples[\"text\"],\n                            examples[\"type\"],\n                            examples[\"TARGET\"]),\n                       key=lambda x: (x[0], x[1]))\n    dialogues = []\n    current_dialogue = []\n    current_id = None\n    for item in sorted_data:\n        dialogue_id, turn_id, text, turn_type, target = item\n        if dialogue_id != current_id:\n            if current_id is not None and current_dialogue:\n                dialogues.append({\n                    \"dialogue_id\": current_id,\n                    \"turns\": current_dialogue,\n                    \"target\": current_dialogue[0][\"target\"]\n                })\n            current_id = dialogue_id\n            current_dialogue = []\n        current_dialogue.append({\n            \"text\": text,\n            \"type\": turn_type,\n            \"target\": target\n        })\n    if current_id is not None and current_dialogue:\n        dialogues.append({\n            \"dialogue_id\": current_id,\n            \"turns\": current_dialogue,\n            \"target\": current_dialogue[0][\"target\"]\n        })\n    return {\"dialogues\": dialogues}\n\nprocessed_dataset = dataset.map(\n    group_dialogues,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    batch_size=1000\n)\n\n# 3. Create instruction-tuning format for each CN turn\ndef create_instruction_examples(examples):\n    new_examples = {\"input\": [], \"target\": []}\n    for dialogue in examples[\"dialogues\"]:\n        history = []\n        for turn in dialogue[\"turns\"]:\n            if turn[\"type\"] == \"CN\":\n                # Instruction-style prompt\n                prompt = (\n                    \"Instruction: Given the following conversation, generate a fact-based counterspeech response.\\n\"\n                    \"Conversation: \" + \" [SEP] \".join(history) +\n                    \"\\nResponse:\"\n                )\n                new_examples[\"input\"].append(prompt)\n                new_examples[\"target\"].append(turn[\"text\"])\n            history.append(turn[\"text\"])\n    return new_examples\n\nfinal_dataset = processed_dataset.map(\n    create_instruction_examples,\n    batched=True,\n    remove_columns=[\"dialogues\"]\n)\n\n# 4. Load pretrained FLAN-T5-XL model and tokenizer\nmodel_name = \"google/flan-t5-xl\"\ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = T5ForConditionalGeneration.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n\n# 5. Apply LoRA\nlora_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    target_modules=[\"q\", \"k\", \"v\"],  # T5 attention modules\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=TaskType.SEQ_2_SEQ_LM\n)\nmodel = get_peft_model(model, lora_config)\nmodel.print_trainable_parameters()\n\n# 6. Tokenization for Trainer\ndef preprocess_function(examples):\n    model_inputs = tokenizer(\n        examples[\"input\"],\n        max_length=128,\n        truncation=True,\n        padding=\"max_length\"\n    )\n    labels = tokenizer(\n        examples[\"target\"],\n        max_length=64,\n        truncation=True,\n        padding=\"max_length\"\n    )[\"input_ids\"]\n    labels = [\n        [(label if label != tokenizer.pad_token_id else -100) for label in label_seq]\n        for label_seq in labels\n    ]\n    model_inputs[\"labels\"] = labels\n    return model_inputs\n\ntokenized_datasets = final_dataset.map(\n    preprocess_function,\n    batched=True,\n    remove_columns=[\"input\", \"target\"]\n)\n\n# 7. TrainingArguments and Trainer\ntraining_args = TrainingArguments(\n    output_dir=\"./flan-t5-xl-lora-instruction-counterspeech\",\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    num_train_epochs=2,\n    fp16=False,  # Use fp16=True only if you have stability and hardware for it\n    eval_strategy=\"steps\",\n    eval_steps=500,\n    save_strategy=\"steps\",\n    save_steps=500,\n    logging_strategy=\"steps\",\n    logging_steps=10,\n    report_to=\"none\",\n    learning_rate=2e-4\n)\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer\n)\n\nprint(\"Starting instruction-tuned LoRA fine-tuning...\")\ntrainer.train()\n\n# 8. Generation function (inference after fine-tuning)\ndef generate_counterspeech(dialogue_history):\n    device = model.device\n    input_text = (\n        \"Instruction: Given the following conversation, generate a fact-based counterspeech response.\\n\"\n        \"Conversation: \" + \" [SEP] \".join(dialogue_history) +\n        \"\\nResponse:\"\n    )\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            input_ids=inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            max_new_tokens=128,\n            num_beams=5,\n            repetition_penalty=2.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 9. Example usage\nsample_dialogue = [\n    \"You people are ruining our country!\",\n    \"Immigrants are stealing our jobs!\",\n    \"We should send them all back!\"\n]\nprint(\"\\nGenerated counterspeech (FLAN-T5-XL + LoRA, instruction-tuned):\")\nprint(generate_counterspeech(sample_dialogue))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:02:05.738260Z","iopub.execute_input":"2025-05-08T14:02:05.738543Z","iopub.status.idle":"2025-05-08T14:21:05.469769Z","shell.execute_reply.started":"2025-05-08T14:02:05.738519Z","shell.execute_reply":"2025-05-08T14:21:05.469060Z"}},"outputs":[{"name":"stderr","text":"2025-05-08 14:02:10.848303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746712930.871473     312 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746712930.878388     312 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nYou are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"120991dc12b5489196df544e7f8aaea2"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_312/2854361132.py:145: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 14,155,776 || all params: 2,863,912,960 || trainable%: 0.4943\nStarting instruction-tuned LoRA fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2978' max='2978' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2978/2978 18:40, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>2.802500</td>\n      <td>2.558594</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>2.528900</td>\n      <td>2.535156</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>2.412400</td>\n      <td>2.505859</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>2.612000</td>\n      <td>2.490234</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>2.128900</td>\n      <td>2.486328</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"\nGenerated counterspeech (FLAN-T5-XL + LoRA, instruction-tuned):\nI don't think it's fair to say that immigrants are stealing our jobs.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def generate_counterspeech(dialogue_history):\n    device = model.device\n    input_text = \"Given the following conversation, generate a fact-based counterspeech response:\\n\" + \" [SEP] \".join(dialogue_history)\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=128, truncation=True).to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            input_ids=inputs.input_ids,\n            attention_mask=inputs.attention_mask,\n            max_new_tokens=128,\n            num_beams=5,\n            repetition_penalty=2.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n# 9. Example usage\nsample_dialogue = [\n    \"You people are ruining our country!\",\n    \"Immigrants are stealing our jobs!\",\n    \"We should send them all back!\"\n]\nprint(\"\\nGenerated counterspeech (FLAN-T5-XL + LoRA):\")\nprint(generate_counterspeech(sample_dialogue))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:21:10.822216Z","iopub.execute_input":"2025-05-08T14:21:10.822519Z","iopub.status.idle":"2025-05-08T14:21:11.835377Z","shell.execute_reply.started":"2025-05-08T14:21:10.822497Z","shell.execute_reply":"2025-05-08T14:21:11.834721Z"}},"outputs":[{"name":"stdout","text":"\nGenerated counterspeech (FLAN-T5-XL + LoRA):\nThey are not stealing our jobs, they are contributing to our economy.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install dependencies if not already installed\n!pip install -q evaluate detoxify tqdm\n!pip install rouge_score bert_score\nfrom evaluate import load\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\n# Load metrics\nrouge = load(\"rouge\")\nbertscore = load(\"bertscore\")\n# Use first 100 samples\ninputs = [ex[\"input\"] for ex in final_dataset[\"validation\"]][:100]\ntargets = [ex[\"target\"] for ex in final_dataset[\"validation\"]][:100]\n\n# Generate predictions\nprint(\"Generating counter speech...\")\ngenerated = []\nfor text in tqdm(inputs, desc=\"Generating\"):\n    response = generate_counterspeech(text)  # <-- make sure this function is defined\n    generated.append(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:21:20.849321Z","iopub.execute_input":"2025-05-08T14:21:20.849818Z","iopub.status.idle":"2025-05-08T14:29:38.053956Z","shell.execute_reply.started":"2025-05-08T14:21:20.849786Z","shell.execute_reply":"2025-05-08T14:29:38.053153Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nGenerating counter speech...\n","output_type":"stream"},{"name":"stderr","text":"Generating: 100%|██████████| 100/100 [08:09<00:00,  4.89s/it]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# BERTScore\n#print(\"Calculating BERTScore...\")\nbertscore_result = bertscore.compute(\n    predictions=generated,\n    references=targets,\n    model_type=\"distilbert-base-uncased\"\n)\nprint(f\"BERTScore F1: {np.mean(bertscore_result['f1']):.4f}\")\n\n# ROUGE\n# ROUGE\n#print(\"Calculating ROUGE...\")\nrouge_result = rouge.compute(predictions=generated, references=targets)\nprint(f\"ROUGE-1 F1: {rouge_result['rouge1']:.4f}\")\nprint(f\"ROUGE-2 F1: {rouge_result['rouge2']:.4f}\")\nprint(f\"ROUGE-L F1: {rouge_result['rougeL']:.4f}\")\n# Perplexity\n#print(\"Calculating Perplexity...\")\ndef calculate_perplexity(texts):\n    total_log_prob = 0.0\n    total_words = 0\n    for text in texts:\n        words = text.split()\n        total_words += len(words)\n        # You can use a pre-trained language model (e.g., GPT-2) for calculating perplexity\n        # Here, we will use a placeholder for the log-prob calculation, which should ideally come from a language model\n        # For simplicity, assume a fixed value here\n        total_log_prob += len(words) * math.log(1.0)  # Placeholder for log-prob calculation\n    return math.exp(-total_log_prob / total_words) if total_words > 0 else float('inf')\n\nperplexity_result = calculate_perplexity(generated)\nprint(f\"Perplexity: {perplexity_result:.4f}\")\n\n# Toxicity\nprint(\"Calculating Toxicity...\")\ntoxicity_scores = [Detoxify('original').predict(pred)['toxicity'] for pred in tqdm(generated, desc=\"Toxicity\")]\navg_toxicity = np.mean(toxicity_scores)\nprint(f\"Avg. Toxicity Score: {avg_toxicity:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T14:29:53.831465Z","iopub.execute_input":"2025-05-08T14:29:53.832165Z","iopub.status.idle":"2025-05-08T14:30:51.337784Z","shell.execute_reply.started":"2025-05-08T14:29:53.832132Z","shell.execute_reply":"2025-05-08T14:30:51.336905Z"}},"outputs":[{"name":"stdout","text":"BERTScore F1: 0.6208\nROUGE-1 F1: 0.0382\nROUGE-2 F1: 0.0003\nROUGE-L F1: 0.0334\nPerplexity: 1.0000\nCalculating Toxicity...\n","output_type":"stream"},{"name":"stderr","text":"Toxicity: 100%|██████████| 100/100 [00:54<00:00,  1.82it/s]","output_type":"stream"},{"name":"stdout","text":"Avg. Toxicity Score: 0.0242\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4}]}