{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11488033,"sourceType":"datasetVersion","datasetId":7200788},{"sourceId":11488038,"sourceType":"datasetVersion","datasetId":7200792},{"sourceId":11488144,"sourceType":"datasetVersion","datasetId":7200876}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets evaluate bert_score detoxify","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:06:13.974885Z","iopub.execute_input":"2025-05-08T08:06:13.975207Z","iopub.status.idle":"2025-05-08T08:06:17.433157Z","shell.execute_reply.started":"2025-05-08T08:06:13.975186Z","shell.execute_reply":"2025-05-08T08:06:17.432488Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom datasets import load_dataset, DatasetDict\nimport numpy as np\nfrom evaluate import load\n\n# 1. Load and prepare dataset\ndataset = load_dataset(\"Rhma/DIALOCONAN\")\nsmall_dataset = dataset[\"train\"].select(range(3500))\n\n# Split train data into train/validation\ntrain_val = small_dataset.train_test_split(test_size=0.15, seed=42)\ndataset = DatasetDict({\n    \"train\": train_val[\"train\"],\n    \"validation\": train_val[\"test\"]\n})\n\n# 2. Group turns by dialogue_id\ndef group_dialogues(examples):\n    sorted_data = sorted(zip(examples[\"dialogue_id\"], \n                            examples[\"turn_id\"], \n                            examples[\"text\"],\n                            examples[\"type\"],\n                            examples[\"TARGET\"]),\n                       key=lambda x: (x[0], x[1]))\n    dialogues = []\n    current_dialogue = []\n    current_id = None\n    for item in sorted_data:\n        dialogue_id, turn_id, text, turn_type, target = item\n        if dialogue_id != current_id:\n            if current_id is not None and current_dialogue:\n                dialogues.append({\n                    \"dialogue_id\": current_id,\n                    \"turns\": current_dialogue,\n                    \"target\": current_dialogue[0][\"target\"]\n                })\n            current_id = dialogue_id\n            current_dialogue = []\n        current_dialogue.append({\n            \"text\": text,\n            \"type\": turn_type,\n            \"target\": target\n        })\n    if current_id is not None and current_dialogue:\n        dialogues.append({\n            \"dialogue_id\": current_id,\n            \"turns\": current_dialogue,\n            \"target\": current_dialogue[0][\"target\"]\n        })\n    return {\"dialogues\": dialogues}\n\nprocessed_dataset = dataset.map(\n    group_dialogues,\n    batched=True,\n    remove_columns=dataset[\"train\"].column_names,\n    batch_size=1000\n)\n\n# 3. Create conversation history for each CN turn\ndef create_conversation_history(examples):\n    new_examples = {\"input\": [], \"target\": []}\n    for dialogue in examples[\"dialogues\"]:\n        history = []\n        for turn in dialogue[\"turns\"]:\n            if turn[\"type\"] == \"CN\":\n                new_examples[\"input\"].append(\" [SEP] \".join(history))\n                new_examples[\"target\"].append(turn[\"text\"])\n            history.append(turn[\"text\"])\n    return new_examples\n\nfinal_dataset = processed_dataset.map(\n    create_conversation_history,\n    batched=True,\n    remove_columns=[\"dialogues\"]\n)\n\n# 4. Load pretrained BLOOMZ model and tokenizer\nmodel_name = \"bigscience/bloomz-3b\"\ntokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)\ntokenizer.pad_token = tokenizer.eos_token\nmodel = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\", torch_dtype=torch.float16)\n\n# 5. Generation function (inference only)\ndef generate_counterspeech(dialogue_history):\n    device = model.device\n    input_text = \" [SEP] \".join(dialogue_history) + \" [ANS] \"\n    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True).to(device)\n    with torch.inference_mode():\n        outputs = model.generate(\n            inputs.input_ids,\n            max_new_tokens=128,\n            num_beams=5,\n            repetition_penalty=2.0,\n            early_stopping=True\n        )\n    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# 6. Example usage\nsample_dialogue = [\n    \"You people are ruining our country!\",\n    \"Immigrants are stealing our jobs!\",\n    \"We should send them all back!\"\n]\nprint(\"\\nGenerated counterspeech:\")\nprint(generate_counterspeech(sample_dialogue))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:06:17.451309Z","iopub.execute_input":"2025-05-08T08:06:17.451530Z","iopub.status.idle":"2025-05-08T08:06:58.749740Z","shell.execute_reply.started":"2025-05-08T08:06:17.451503Z","shell.execute_reply":"2025-05-08T08:06:58.748900Z"}},"outputs":[{"name":"stderr","text":"2025-05-08 08:06:22.847056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746691582.869404     136 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746691582.876300     136 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/452 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b270c483b960499a805765b868bd0a51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/1.16M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ac404677c2c40a3b34ff7b8eaf04abd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16625 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31ff4afe8d1a4c8282f29fc16db89e9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2975 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"677a37868a3e4cd2a8a2bc9dfd0f0527"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/525 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"da6acb6096584682b7ed791d54914218"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1616 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11262c357dd746b1900a5097976a6abd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/381 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9e3e2853c94cbbbab76c531a8d0d09"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/199 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95dbc605c43e4675b29220d34667a17c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/14.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dabd1419ad24c94b2b86964b8f4815c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff1fe034c07404eac9ce32a11b48291"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/715 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b97d0089302492480c3faa8662aae7f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/6.01G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e333c3c4287425aa4485fcbc5d2ce07"}},"metadata":{}},{"name":"stdout","text":"\nGenerated counterspeech:\nYou people are ruining our country! [SEP] Immigrants are stealing our jobs! [SEP] We should send them all back! [ANS]  No, we will not send them back.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# Install dependencies if not already installed\n!pip install -q evaluate detoxify tqdm\n\nfrom evaluate import load\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\nimport numpy as np\n\n# Load metrics\nbertscore = load(\"bertscore\")\nmeteor = load(\"meteor\")\nbleu = load(\"bleu\")\n\n# Use first 100 samples\ninputs = [ex[\"input\"] for ex in final_dataset[\"validation\"]][:100]\ntargets = [ex[\"target\"] for ex in final_dataset[\"validation\"]][:100]\n\n# Generate predictions\nprint(\"Generating counter speech...\")\ngenerated = []\nfor text in tqdm(inputs, desc=\"Generating\"):\n    response = generate_counterspeech(text)  # <-- make sure this function is defined\n    generated.append(response)\n\n# BERTScore\nprint(\"Calculating BERTScore...\")\nbertscore_result = bertscore.compute(\n    predictions=generated,\n    references=targets,\n    model_type=\"distilbert-base-uncased\"\n)\nprint(f\"BERTScore F1: {np.mean(bertscore_result['f1']):.4f}\")\n\n# METEOR\nprint(\"Calculating METEOR...\")\nmeteor_result = meteor.compute(predictions=generated, references=targets)\nprint(f\"METEOR: {meteor_result['meteor']:.4f}\")\n\n# BLEU\nprint(\"Calculating BLEU...\")\nbleu_result = bleu.compute(predictions=generated, references=[[ref] for ref in targets])\nprint(f\"BLEU: {bleu_result['bleu']:.4f}\")\n\n# Toxicity\nprint(\"Calculating Toxicity...\")\ntoxicity_scores = [Detoxify('original').predict(pred)['toxicity'] for pred in tqdm(generated, desc=\"Toxicity\")]\navg_toxicity = np.mean(toxicity_scores)\nprint(f\"Avg. Toxicity Score: {avg_toxicity:.4f}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install dependencies if not already installed\n!pip install -q evaluate detoxify tqdm\n!pip install rouge_score bert_score\nfrom evaluate import load\nfrom detoxify import Detoxify\nfrom tqdm import tqdm\nimport numpy as np\nimport math\n\n# Load metrics\nrouge = load(\"rouge\")\nbertscore = load(\"bertscore\")\n# Use first 100 samples\ninputs = [ex[\"input\"] for ex in final_dataset[\"validation\"]][:100]\ntargets = [ex[\"target\"] for ex in final_dataset[\"validation\"]][:100]\n\n# Generate predictions\nprint(\"Generating counter speech...\")\ngenerated = []\nfor text in tqdm(inputs, desc=\"Generating\"):\n    response = generate_counterspeech(text)  # <-- make sure this function is defined\n    generated.append(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:07:16.216638Z","iopub.execute_input":"2025-05-08T08:07:16.217218Z","iopub.status.idle":"2025-05-08T08:14:22.218281Z","shell.execute_reply.started":"2025-05-08T08:07:16.217191Z","shell.execute_reply":"2025-05-08T08:14:22.217208Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\nRequirement already satisfied: bert_score in /usr/local/lib/python3.11/dist-packages (0.3.13)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.5.1+cu124)\nRequirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.2.3)\nRequirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.51.1)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from bert_score) (2.32.3)\nRequirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.11/dist-packages (from bert_score) (4.67.1)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from bert_score) (3.7.5)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bert_score) (24.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rouge_score) (2.4.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (4.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.3.1.170)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (12.4.127)\nRequirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (3.1.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.0.0->bert_score) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.0.0->bert_score) (1.3.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.30.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=3.0.0->bert_score) (0.5.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (11.1.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->bert_score) (3.2.1)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.4.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->bert_score) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rouge_score) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rouge_score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rouge_score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rouge_score) (2024.2.0)\nGenerating counter speech...\n","output_type":"stream"},{"name":"stderr","text":"Generating: 100%|██████████| 100/100 [06:51<00:00,  4.12s/it]","output_type":"stream"},{"name":"stdout","text":"Calculating BERTScore...\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"233c2e8b166d4688bc19a7faef36ffb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f03d4ed1707d43938cae8da0aac4c00d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51f373a2451249daa2a2836a49864002"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0703ce0d8cc94978b9f006860176eb11"}},"metadata":{}},{"name":"stderr","text":"Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c778ab2812a44a65a03ddda531852074"}},"metadata":{}},{"name":"stdout","text":"BERTScore F1: 0.5972\nCalculating ROUGE...\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_136/1325122679.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Calculating ROUGE...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0mrouge_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrouge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ROUGE-1: {rouge_result['rouge1'].fmeasure:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ROUGE-2: {rouge_result['rouge2'].fmeasure:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"ROUGE-L: {rouge_result['rougeL'].fmeasure:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'numpy.float64' object has no attribute 'fmeasure'"],"ename":"AttributeError","evalue":"'numpy.float64' object has no attribute 'fmeasure'","output_type":"error"}],"execution_count":3},{"cell_type":"code","source":"# BERTScore\nprint(\"Calculating BERTScore...\")\nbertscore_result = bertscore.compute(\n    predictions=generated,\n    references=targets,\n    model_type=\"distilbert-base-uncased\"\n)\nprint(f\"BERTScore F1: {np.mean(bertscore_result['f1']):.4f}\")\n\n# ROUGE\n# ROUGE\nprint(\"Calculating ROUGE...\")\nrouge_result = rouge.compute(predictions=generated, references=targets)\nprint(f\"ROUGE-1 F1: {rouge_result['rouge1']:.4f}\")\nprint(f\"ROUGE-2 F1: {rouge_result['rouge2']:.4f}\")\nprint(f\"ROUGE-L F1: {rouge_result['rougeL']:.4f}\")\n# Perplexity\nprint(\"Calculating Perplexity...\")\ndef calculate_perplexity(texts):\n    total_log_prob = 0.0\n    total_words = 0\n    for text in texts:\n        words = text.split()\n        total_words += len(words)\n        # You can use a pre-trained language model (e.g., GPT-2) for calculating perplexity\n        # Here, we will use a placeholder for the log-prob calculation, which should ideally come from a language model\n        # For simplicity, assume a fixed value here\n        total_log_prob += len(words) * math.log(1.0)  # Placeholder for log-prob calculation\n    return math.exp(-total_log_prob / total_words) if total_words > 0 else float('inf')\n\nperplexity_result = calculate_perplexity(generated)\nprint(f\"Perplexity: {perplexity_result:.4f}\")\n\n# Toxicity\nprint(\"Calculating Toxicity...\")\ntoxicity_scores = [Detoxify('original').predict(pred)['toxicity'] for pred in tqdm(generated, desc=\"Toxicity\")]\navg_toxicity = np.mean(toxicity_scores)\nprint(f\"Avg. Toxicity Score: {avg_toxicity:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T08:34:07.041758Z","iopub.execute_input":"2025-05-08T08:34:07.042244Z","iopub.status.idle":"2025-05-08T08:34:53.510729Z","shell.execute_reply.started":"2025-05-08T08:34:07.042218Z","shell.execute_reply":"2025-05-08T08:34:53.509935Z"}},"outputs":[{"name":"stdout","text":"Calculating BERTScore...\nBERTScore F1: 0.5972\nCalculating ROUGE...\nROUGE-1 F1: 0.0332\nROUGE-2 F1: 0.0005\nROUGE-L F1: 0.0307\nCalculating Perplexity...\nPerplexity: 1.0000\nCalculating Toxicity...\n","output_type":"stream"},{"name":"stderr","text":"Toxicity:   0%|          | 0/100 [00:00<?, ?it/s]Downloading: \"https://github.com/unitaryai/detoxify/releases/download/v0.1-alpha/toxic_original-c1212f89.ckpt\" to /root/.cache/torch/hub/checkpoints/toxic_original-c1212f89.ckpt\n\n  0%|          | 0.00/418M [00:00<?, ?B/s]\u001b[A\n  3%|▎         | 14.4M/418M [00:00<00:02, 150MB/s]\u001b[A\n 12%|█▏        | 50.1M/418M [00:00<00:01, 278MB/s]\u001b[A\n 21%|██        | 85.8M/418M [00:00<00:01, 321MB/s]\u001b[A\n 28%|██▊       | 116M/418M [00:00<00:01, 287MB/s] \u001b[A\n 35%|███▍      | 144M/418M [00:00<00:01, 256MB/s]\u001b[A\n 41%|████▏     | 173M/418M [00:00<00:00, 267MB/s]\u001b[A\n 50%|████▉     | 208M/418M [00:00<00:00, 297MB/s]\u001b[A\n 57%|█████▋    | 238M/418M [00:00<00:00, 288MB/s]\u001b[A\n 65%|██████▌   | 274M/418M [00:00<00:00, 313MB/s]\u001b[A\n 73%|███████▎  | 304M/418M [00:01<00:00, 302MB/s]\u001b[A\n 82%|████████▏ | 341M/418M [00:01<00:00, 327MB/s]\u001b[A\n 89%|████████▉ | 373M/418M [00:01<00:00, 323MB/s]\u001b[A\n100%|██████████| 418M/418M [00:01<00:00, 278MB/s]\u001b[A\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1c99ef28754424c9c52b3457dd0e418"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3039f016c6f44d8cb2b6e5fb7d1514bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"830449705f3c46a28047e3e04f76163f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82217f86b5af47c990460ffd87c0b20b"}},"metadata":{}},{"name":"stderr","text":"Toxicity: 100%|██████████| 100/100 [00:45<00:00,  2.19it/s]","output_type":"stream"},{"name":"stdout","text":"Avg. Toxicity Score: 0.0020\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":4}]}